# === HYPERSPACE DB CONFIGURATION ===

# --- Server Dimensions & Metric ---
# Option 1: Legacy / Classical RAG (OpenAI, BGE-M3)
HS_DIMENSION=1536
HS_METRIC=l2

# Option 2: Scientific / Native Hyperbolic
# Use 'poincare' for maximum efficiency on compressed hyperbolic vectors.
# HS_DIMENSION=64
# HS_METRIC=poincare

# Legacy alias support (do not remove unless sure)
# HS_DISTANCE_METRIC=poincare

# --- HNSW Engine Tuning ---
HS_HNSW_M=16
HS_HNSW_EF_CONSTRUCT=100
HS_HNSW_EF_SEARCH=50

# --- Durability & Safety ---
# Options: async (default), batch (100ms lag), strict (fsync every write)
# Controls the trade-off between write speed and data safety.
HYPERSPACE_WAL_SYNC_MODE=async
# Interval for Batch mode in milliseconds
HYPERSPACE_WAL_BATCH_INTERVAL=100

# --- Storage & Quantization ---
# Options: none, scalar (i8), binary (1 bit)
# Note: Scalar quantization clamps values to [-1, 1]. Use 'none' for unbounded L2 vectors if you care about magnitude > 1.
HS_QUANTIZATION_LEVEL=none

# --- Security ---
HYPERSPACE_API_KEY=I_LOVE_HYPERSPACEDB

# === EMBEDDING PIPELINE CONFIGURATION ===
# Provide vectors automatically (InsertText).
HYPERSPACE_EMBED=False

# Provider: 'local' (ONNX), 'openai', 'cohere', 'mistral', 'voyage', 'gemini', 'openrouter'
HYPERSPACE_EMBED_PROVIDER=local

# --- Local ONNX Model (Provider=local) ---
# Path to .onnx file
# HYPERSPACE_MODEL_PATH=./models/v5_Embedding_128d.onnx
# HYPERSPACE_TOKENIZER_PATH=./models/v5_Embedding_Tokenizer.json
# HYPERSPACE_EMBED_DIM=128

# --- Remote API (Provider=openai/cohere/etc) ---
# HYPERSPACE_API_KEY_EMBED=sk-... (Or use OPENAI_API_KEY)
# HYPERSPACE_EMBED_MODEL=text-embedding-3-small
# HYPERSPACE_API_BASE=... (Optional base URL override)
